#!/usr/bin/env python

from Coach import Coach
from games.tictactoe.TicTacToeGame import TicTacToeGame as Game
from games.tictactoe.mxnet.NNet import NNetWrapper as nn
from utils import *
import numpy
import GA
from random import shuffle
import math
import os

train_examples_file_name = "checkpoint.examples"

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
channel_name='training'
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'io/config/hyperparameters.json')
input_path = output_path = os.path.join(prefix + 'io/data', channel_name)

# Read in any hyperparameters that the user passed with the training job
if os.path.isfile(param_path):
    with open(param_path, 'r') as tc:
        args = dotdict(json.load(tc))

args = dotdict({
    'numIters': 2,
    'numEps': 20,
    'tempThreshold': 15,
    'updateThreshold': 0.6,
    'maxlenOfQueue': 200000,
    'numMCTSSims': 25,
    'arenaCompare': 4,
    'cpuct': 1,

    'checkpoint': './temp/',
    'load_model': False,
    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),
    'numItersForTrainExamplesHistory': 7,
})


def train():
    """
    Genetic algorithm parameters:
        Mating pool size
        Population size
    """
    sol_per_pop = 6
    num_parents = 3
    num_generations = 20

    """
    Creating the initial population.
        Generate population from scratch if there is no checkpoint,
        or load population from file.
    """
    #
    input_model_files = [ [model_path, file] for file in os.listdir(model_path) ]
    input_model_files.sort()
    new_population = []
    game = Game()
    if len(input_model_files) == 0:
        for i in range(0,sol_per_pop):
            new_population.append(nn(game))
    else:
        print("Checkpoint File found. Read it.")
        sol_per_pop = len(input_model_files)
        for load_model_file in input_model_files:
            print(os.path.join(load_model_file[0], load_model_file[1]))
            nnet = nn(game)
            nnet.load_checkpoint(load_model_file[0], load_model_file[1])
            new_population.append(nnet)

    """
    Creating Coach class used to generate Training Example
        Load Train Example History if exist
    """
    master = Coach(Game(), args)
    examples_file_path = os.path.join(input_path, train_examples_file_name)
    if os.path.isfile(examples_file_path) :
        print(os.path.join(examples_file_path))
        master.loadTrainExamples(examples_file_path)
    alpha_index = 0
    """
    Start Genetic Algorithm
    """
    ancestors = list(range(0,sol_per_pop))
    for generation in range(num_generations):
        # Generate Train Examples by using alpha chromosome
        alpha_padawan = new_population[alpha_index] # Select Alpha Padawan, usually is the first in list.
        train_examples = master.generate(alpha_padawan)

        # Training each chromosome in the population osing Train Examples.
        master.train(new_population, train_examples)

        # Measing the fitness of each chromosome in the population.
        fitness = GA.cal_pop_fitness(new_population, args)

        # Selecting the best parents in the population for mating.
        nnet_parents, parents_weights, indices = GA.select_mating_pool(new_population, fitness, num_parents)

        # Generating next generation using crossover.
        offspring_crossover = GA.crossover(parents_weights, offspring_size=sol_per_pop-num_parents)

        # Adding some variations to the offsrping using mutation.
        offspring_crossover = GA.mutation(offspring_crossover)
        nnet_offspring_mutation = []
        for m in offspring_crossover:
            nnet = nn(game)
            nnet.set_weights(m)
            nnet_offspring_mutation.append(nnet)

        # Creating the new population based on the parents and offspring.
        new_population[0:num_parents] = nnet_parents
        new_population[num_parents:] = nnet_offspring_mutation

        # Save Current State
        print("Save Checkpoint")
        new_population[0].save_checkpoint(model_path, 'alpha.network')
        for iidx, nnet in enumerate(new_population[1:]):
            nnet.save_checkpoint(model_path, 'beta{}.network'.format(iidx))
        print("Save Train Examples")
        master.saveTrainExamples(output_path, train_examples_file_name)

        # Print current results
        lossness = [p.get_loss() for p in nnet_parents]
        ancestors = [indices.index(i) for i in set(ancestors).intersection(indices)]
        print("Generation : {} | Best result : {} | Ancestors {}".format(generation, min(lossness)[0], ancestors) )
        print('>Alpha Padawan ', nnet_parents[0].name)
        for m in nnet_parents[1:]:
            print('Senior Padawan ', m.name)
        for m in nnet_offspring_mutation:
            print('Junior Padawan ', m.name)
        #args.arenaCompare = args.arenaCompare + int(math.sqrt(generation))
        if (len(ancestors) == 0):
            print("All ancestors are dead, generating..")
            ancestors = list(range(0,num_parents))
        alpha_index = 0

    # Getting the best solution after iterating finishing all generations.
    #At first, the fitness is calculated for each solution in the final generation.
    fitness = GA.cal_pop_fitness(new_population, args)
    # Then return the index of that solution corresponding to the best fitness.
    best_match_idx = numpy.where(fitness == numpy.max(fitness))

    print("Best solution : ", new_population[best_match_idx])
    print("Best solution fitness : ", fitness[best_match_idx])

if __name__ == '__main__':
    train()
